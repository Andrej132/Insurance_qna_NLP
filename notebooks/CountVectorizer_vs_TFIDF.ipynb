{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e49e07-c725-4366-953b-11e47dc55811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "sys.path.append(\"../data\")\n",
    "from test_data import original, easy, medium, hard\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm.notebook import tqdm\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9294245e-9fb9-4508-92ef-347cde2e49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fe4e43-3f29-4b03-987f-4ded6e1e2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_set(data):\n",
    "    lemmatized = []\n",
    "    for sentence in data:\n",
    "        tokens = nltk.word_tokenize(sentence.lower())\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "        lemm = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        lemmatized.append(' '.join(lemm))\n",
    "    return lemmatized\n",
    "\n",
    "def stemmed_set(data):\n",
    "    stemmed = []\n",
    "    for sentence in data:\n",
    "        tokens = nltk.word_tokenize(sentence.lower())\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "        stemms = [stemmer.stem(t) for t in tokens]\n",
    "        stemmed.append(' '.join(stemms))\n",
    "    return stemmed\n",
    "\n",
    "def train_knn(train_vectors, metric):\n",
    "    return NearestNeighbors(n_neighbors=100, metric=metric).fit(train_vectors)\n",
    "\n",
    "def k_nearest(model, matrix):\n",
    "    distance, indices = model.kneighbors(matrix.reshape(1,-1))\n",
    "    return distance, indices\n",
    "\n",
    "def ranking(model, test_data, corpus):\n",
    "    total_rank = 0\n",
    "    for elem in test_data:\n",
    "        changed_question, original = elem[0], elem[1]\n",
    "        _, indices = k_nearest(model, changed_question)\n",
    "        results = corpus[indices[0]]\n",
    "        for i, question in enumerate(results):\n",
    "            if original == question:\n",
    "                total_rank += i\n",
    "                break\n",
    "        if i == 99:\n",
    "            total_rank += 200\n",
    "    avg_rank = total_rank / len(test_data)\n",
    "    return avg_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6939eb-2379-4458-a353-371041ac0496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What Happens When Term Life Insurance Is Paid ...</td>\n",
       "      <td>Actually term life insurance cannot be paid up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What Happens When Term Life Insurance Is Paid ...</td>\n",
       "      <td>Term life insurance is never paid up. Assuming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What Happens When Term Life Insurance Is Paid ...</td>\n",
       "      <td>Term Life Insurance does not have the option t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What Does Renters Insurance Cover?</td>\n",
       "      <td>A renters insurance policy will typically prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What Does Renters Insurance Cover?</td>\n",
       "      <td>If you apartment was on fire and all your pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Question  \\\n",
       "0           0  What Happens When Term Life Insurance Is Paid ...   \n",
       "1           1  What Happens When Term Life Insurance Is Paid ...   \n",
       "2           2  What Happens When Term Life Insurance Is Paid ...   \n",
       "3           3                What Does Renters Insurance Cover?    \n",
       "4           4                What Does Renters Insurance Cover?    \n",
       "\n",
       "                                              Answer  \n",
       "0  Actually term life insurance cannot be paid up...  \n",
       "1  Term life insurance is never paid up. Assuming...  \n",
       "2  Term Life Insurance does not have the option t...  \n",
       "3  A renters insurance policy will typically prov...  \n",
       "4  If you apartment was on fire and all your pers...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train = pd.read_csv('../data/insurance_qna_dataset.csv', delimiter='\\t')\n",
    "corpus_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84049eb5-eb4a-4800-aba9-4caf92cc727d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What Happens When Term Life Insurance Is Paid Up',\n",
       " 'What Does Renters Insurance Cover',\n",
       " 'Does Owning A Pitbull Raise Homeowners Insurance',\n",
       " 'What Should You Look For In Long Term Care Insurance',\n",
       " 'Will Medicare Pay For Smoking Cessation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_questions = corpus_train['Question'].drop_duplicates()\n",
    "corpus_questions_list = [q.strip().translate(str.maketrans(\"\",\"\", string.punctuation)) for q in corpus_questions]\n",
    "corpus_questions_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4883573-f1aa-420d-85fd-7344f77535a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Changed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why Do They Take Blood And Urine For Life Insu...</td>\n",
       "      <td>Why Do They Take Bloods And Urine For Lifes In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What Are Rates For Long Term Care Insurance?</td>\n",
       "      <td>What Are Rate For Long Term Care Insurance?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where Can I Get Weekly Car Insurance?</td>\n",
       "      <td>Where Can I Get Weekly Car Insurances?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who Has The Best Critical Illness Insurance?</td>\n",
       "      <td>Who Has The Best Critical Illness Insurances?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can You Have Car Insurance Without Owning A Car?</td>\n",
       "      <td>Can You Have Cars Insurance Without Owning A Car?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Original  \\\n",
       "0  Why Do They Take Blood And Urine For Life Insu...   \n",
       "1      What Are Rates For Long Term Care Insurance?    \n",
       "2             Where Can I Get Weekly Car Insurance?    \n",
       "3      Who Has The Best Critical Illness Insurance?    \n",
       "4  Can You Have Car Insurance Without Owning A Car?    \n",
       "\n",
       "                                             Changed  \n",
       "0  Why Do They Take Bloods And Urine For Lifes In...  \n",
       "1        What Are Rate For Long Term Care Insurance?  \n",
       "2             Where Can I Get Weekly Car Insurances?  \n",
       "3      Who Has The Best Critical Illness Insurances?  \n",
       "4  Can You Have Cars Insurance Without Owning A Car?  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_questions = original\n",
    "changed_questions_test = easy + medium + hard\n",
    "corpus_test = pd.DataFrame({'Original': 3 * original_questions, 'Changed': changed_questions_test})\n",
    "corpus_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21546d5-14f5-43b6-b232-70f437c8e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test_changed = corpus_test['Changed']\n",
    "test_changed_list = [changed.strip().translate(str.maketrans(\"\",\"\",string.punctuation)) for changed in corpus_test_changed]\n",
    "corpus_test_original = corpus_test['Original']\n",
    "test_original_list = [original.strip().translate(str.maketrans(\"\",\"\",string.punctuation)) for original in corpus_test_original]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "066290f0-b518-4a5c-b7be-b646c47fbfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['cosine', 'manhattan', 'euclidean']\n",
    "\n",
    "vectorizers = [\n",
    "    (\"CountVectorizer Unigram\", CountVectorizer(ngram_range=(1,1))),\n",
    "    (\"CountVectorizer Bigram\", CountVectorizer(ngram_range=(1,2))),\n",
    "    (\"TFIDFVectorizer Unigram\", TfidfVectorizer(ngram_range=(1,1))),\n",
    "    (\"TFIDFVectorizer Bigram\", TfidfVectorizer(ngram_range=(1,2)))\n",
    "]\n",
    "\n",
    "processing_types = [\n",
    "    (\"original\", corpus_questions_list, test_changed_list, test_original_list),\n",
    "    (\"lemmatized\", lemmatized_set(corpus_questions_list), lemmatized_set(test_changed_list), lemmatized_set(test_original_list)),\n",
    "    (\"stemmed\", stemmed_set(corpus_questions_list), stemmed_set(test_changed_list), stemmed_set(test_original_list))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3af0341-dd97-4bc3-a3ae-4b5d558d9662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original | CountVectorizer Unigram | cosine: 44.47\n",
      "original | CountVectorizer Unigram | manhattan: 47.30\n",
      "original | CountVectorizer Unigram | euclidean: 49.77\n",
      "original | CountVectorizer Bigram | cosine: 43.35\n",
      "original | CountVectorizer Bigram | manhattan: 54.18\n",
      "original | CountVectorizer Bigram | euclidean: 54.08\n",
      "original | TFIDFVectorizer Unigram | cosine: 46.55\n",
      "original | TFIDFVectorizer Unigram | manhattan: 119.85\n",
      "original | TFIDFVectorizer Unigram | euclidean: 46.55\n",
      "original | TFIDFVectorizer Bigram | cosine: 46.38\n",
      "original | TFIDFVectorizer Bigram | manhattan: 133.27\n",
      "original | TFIDFVectorizer Bigram | euclidean: 46.38\n",
      "lemmatized | CountVectorizer Unigram | cosine: 57.20\n",
      "lemmatized | CountVectorizer Unigram | manhattan: 47.55\n",
      "lemmatized | CountVectorizer Unigram | euclidean: 50.63\n",
      "lemmatized | CountVectorizer Bigram | cosine: 54.27\n",
      "lemmatized | CountVectorizer Bigram | manhattan: 69.37\n",
      "lemmatized | CountVectorizer Bigram | euclidean: 74.57\n",
      "lemmatized | TFIDFVectorizer Unigram | cosine: 69.65\n",
      "lemmatized | TFIDFVectorizer Unigram | manhattan: 115.72\n",
      "lemmatized | TFIDFVectorizer Unigram | euclidean: 70.42\n",
      "lemmatized | TFIDFVectorizer Bigram | cosine: 69.30\n",
      "lemmatized | TFIDFVectorizer Bigram | manhattan: 130.40\n",
      "lemmatized | TFIDFVectorizer Bigram | euclidean: 73.68\n",
      "stemmed | CountVectorizer Unigram | cosine: 40.42\n",
      "stemmed | CountVectorizer Unigram | manhattan: 46.92\n",
      "stemmed | CountVectorizer Unigram | euclidean: 46.43\n",
      "stemmed | CountVectorizer Bigram | cosine: 53.67\n",
      "stemmed | CountVectorizer Bigram | manhattan: 71.00\n",
      "stemmed | CountVectorizer Bigram | euclidean: 63.15\n",
      "stemmed | TFIDFVectorizer Unigram | cosine: 70.77\n",
      "stemmed | TFIDFVectorizer Unigram | manhattan: 113.78\n",
      "stemmed | TFIDFVectorizer Unigram | euclidean: 71.25\n",
      "stemmed | TFIDFVectorizer Bigram | cosine: 66.72\n",
      "stemmed | TFIDFVectorizer Bigram | manhattan: 126.27\n",
      "stemmed | TFIDFVectorizer Bigram | euclidean: 70.70\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for name, train_set, test_set, orig_set in processing_types:\n",
    "    for vect_name, vectorizer in vectorizers:\n",
    "        vectorized_train = vectorizer.fit_transform(train_set)\n",
    "        vectorized_test = vectorizer.transform(test_set)\n",
    "        test_data = list(zip(vectorized_test.toarray(), orig_set))\n",
    "        for metric in metrics:\n",
    "            model = train_knn(vectorized_train, metric)\n",
    "            score = ranking(model, test_data, np.array(train_set))\n",
    "            all_results.append({\n",
    "                \"Processing\" : name,\n",
    "                \"Vectorizer\" : vect_name,\n",
    "                \"Distance\" : metric,\n",
    "                \"Avg rank\" : score\n",
    "            })\n",
    "            print(f\"{name} | {vect_name} | {metric}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59a19e8-a69f-4653-93e5-d603ff862494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance                               cosine  euclidean   manhattan\n",
      "Processing Vectorizer                                               \n",
      "lemmatized CountVectorizer Bigram   54.266667  74.566667   69.366667\n",
      "           CountVectorizer Unigram  57.200000  50.633333   47.550000\n",
      "           TFIDFVectorizer Bigram   69.300000  73.683333  130.400000\n",
      "           TFIDFVectorizer Unigram  69.650000  70.416667  115.716667\n",
      "original   CountVectorizer Bigram   43.350000  54.083333   54.183333\n",
      "           CountVectorizer Unigram  44.466667  49.766667   47.300000\n",
      "           TFIDFVectorizer Bigram   46.383333  46.383333  133.266667\n",
      "           TFIDFVectorizer Unigram  46.550000  46.550000  119.850000\n",
      "stemmed    CountVectorizer Bigram   53.666667  63.150000   71.000000\n",
      "           CountVectorizer Unigram  40.416667  46.433333   46.916667\n",
      "           TFIDFVectorizer Bigram   66.716667  70.700000  126.266667\n",
      "           TFIDFVectorizer Unigram  70.766667  71.250000  113.783333\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "print(df_results.pivot_table(index=['Processing','Vectorizer'], columns='Distance', values='Avg rank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d34908-e772-4dee-93e4-99e718b896cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combinations for every kombination:\n",
      "\n",
      "    Processing               Vectorizer   Distance   Avg rank\n",
      "15  lemmatized   CountVectorizer Bigram     cosine  54.266667\n",
      "13  lemmatized  CountVectorizer Unigram  manhattan  47.550000\n",
      "21  lemmatized   TFIDFVectorizer Bigram     cosine  69.300000\n",
      "18  lemmatized  TFIDFVectorizer Unigram     cosine  69.650000\n",
      "3     original   CountVectorizer Bigram     cosine  43.350000\n",
      "0     original  CountVectorizer Unigram     cosine  44.466667\n",
      "9     original   TFIDFVectorizer Bigram     cosine  46.383333\n",
      "6     original  TFIDFVectorizer Unigram     cosine  46.550000\n",
      "27     stemmed   CountVectorizer Bigram     cosine  53.666667\n",
      "24     stemmed  CountVectorizer Unigram     cosine  40.416667\n",
      "33     stemmed   TFIDFVectorizer Bigram     cosine  66.716667\n",
      "30     stemmed  TFIDFVectorizer Unigram     cosine  70.766667\n"
     ]
    }
   ],
   "source": [
    "best_metrics = df_results.loc[df_results.groupby(['Processing', 'Vectorizer'])[\"Avg rank\"].idxmin()]\n",
    "print(\"Best combinations for every kombination:\\n\")\n",
    "print(best_metrics[[\"Processing\", \"Vectorizer\", \"Distance\", \"Avg rank\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a4cf9c-a275-4304-9791-a753331b16d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination: \n",
      "Processing                    stemmed\n",
      "Vectorizer    CountVectorizer Unigram\n",
      "Distance                       cosine\n",
      "Avg rank                    40.416667\n",
      "Name: 24, dtype: object\n"
     ]
    }
   ],
   "source": [
    "final_best = df_results.loc[df_results[\"Avg rank\"].idxmin()]\n",
    "print(\"Best combination: \")\n",
    "print(final_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
